{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5367c3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.10\r\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe5048e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nvidia-tao in /home/ubuntu/.local/lib/python3.8/site-packages (0.1.24)\n",
      "Requirement already satisfied: docker==4.3.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from nvidia-tao) (4.3.1)\n",
      "Requirement already satisfied: tabulate in /home/ubuntu/.local/lib/python3.8/site-packages (from nvidia-tao) (0.8.10)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from nvidia-tao) (1.14.0)\n",
      "Requirement already satisfied: requests!=2.18.0,>=2.14.2 in /usr/lib/python3/dist-packages (from docker==4.3.1->nvidia-tao) (2.22.0)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from docker==4.3.1->nvidia-tao) (1.3.3)\n",
      "Requirement already satisfied: roboflow in /home/ubuntu/.local/lib/python3.8/site-packages (0.2.14)\n",
      "Requirement already satisfied: kiwisolver==1.3.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from roboflow) (1.3.1)\n",
      "Requirement already satisfied: glob2 in /home/ubuntu/.local/lib/python3.8/site-packages (from roboflow) (0.7)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from roboflow) (4.64.0)\n",
      "Requirement already satisfied: urllib3==1.26.6 in /home/ubuntu/.local/lib/python3.8/site-packages (from roboflow) (1.26.6)\n",
      "Requirement already satisfied: idna==2.10 in /home/ubuntu/.local/lib/python3.8/site-packages (from roboflow) (2.10)\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in /usr/lib/python3/dist-packages (from roboflow) (5.3.1)\n",
      "Requirement already satisfied: wget in /home/ubuntu/.local/lib/python3.8/site-packages (from roboflow) (3.2)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from roboflow) (1.14.0)\n",
      "Requirement already satisfied: opencv-python-headless>=4.5.1.48 in /home/ubuntu/.local/lib/python3.8/site-packages (from roboflow) (4.6.0.66)\n",
      "Requirement already satisfied: pyparsing==2.4.7 in /home/ubuntu/.local/lib/python3.8/site-packages (from roboflow) (2.4.7)\n",
      "Requirement already satisfied: cycler==0.10.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from roboflow) (0.10.0)\n",
      "Requirement already satisfied: matplotlib in /home/ubuntu/.local/lib/python3.8/site-packages (from roboflow) (3.5.2)\n",
      "Requirement already satisfied: chardet==4.0.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from roboflow) (4.0.0)\n",
      "Requirement already satisfied: requests-toolbelt in /home/ubuntu/.local/lib/python3.8/site-packages (from roboflow) (0.9.1)\n",
      "Requirement already satisfied: certifi==2021.5.30 in /home/ubuntu/.local/lib/python3.8/site-packages (from roboflow) (2021.5.30)\n",
      "Requirement already satisfied: Pillow>=7.1.2 in /home/ubuntu/.local/lib/python3.8/site-packages (from roboflow) (9.2.0)\n",
      "Requirement already satisfied: python-dateutil in /home/ubuntu/.local/lib/python3.8/site-packages (from roboflow) (2.8.2)\n",
      "Requirement already satisfied: python-dotenv in /home/ubuntu/.local/lib/python3.8/site-packages (from roboflow) (0.20.0)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from roboflow) (2.22.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /home/ubuntu/.local/lib/python3.8/site-packages (from roboflow) (1.23.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from matplotlib->roboflow) (21.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from matplotlib->roboflow) (4.34.4)\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install nvidia-tao\n",
    "!{sys.executable} -m pip install roboflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54ca9ed4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.6) or chardet (4.0.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "usage: tao [-h]\n",
      "           {list,stop,info,action_recognition,augment,bpnet,classification,converter,detectnet_v2,dssd,efficientdet,emotionnet,faster_rcnn,fpenet,gazenet,gesturenet,heartratenet,intent_slot_classification,lprnet,mask_rcnn,multitask_classification,n_gram,pointpillars,pose_classification,punctuation_and_capitalization,question_answering,retinanet,spectro_gen,speech_to_text,speech_to_text_citrinet,speech_to_text_conformer,ssd,text_classification,token_classification,unet,vocoder,yolo_v3,yolo_v4,yolo_v4_tiny}\n",
      "           ...\n",
      "\n",
      "Launcher for TAO Toolkit.\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "\n",
      "tasks:\n",
      "  {list,stop,info,action_recognition,augment,bpnet,classification,converter,detectnet_v2,dssd,efficientdet,emotionnet,faster_rcnn,fpenet,gazenet,gesturenet,heartratenet,intent_slot_classification,lprnet,mask_rcnn,multitask_classification,n_gram,pointpillars,pose_classification,punctuation_and_capitalization,question_answering,retinanet,spectro_gen,speech_to_text,speech_to_text_citrinet,speech_to_text_conformer,ssd,text_classification,token_classification,unet,vocoder,yolo_v3,yolo_v4,yolo_v4_tiny}\n"
     ]
    }
   ],
   "source": [
    "# ensure that the tao binary was installed, \n",
    "# if it wasn't added to the path\n",
    "\n",
    "\n",
    "# option 1 to test\n",
    "!tao\n",
    "\n",
    "# option 2 to test\n",
    "!/home/ubuntu/.local/bin/tao\n",
    "\n",
    "# If option 2 works but not option 1, simply replace instances of \"!tao\" with \"!/home/ubuntu/.local/bin/tao\" throughout this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6089bb40",
   "metadata": {},
   "source": [
    "## Setup TAO mounts.\n",
    "\n",
    "Since the TAO Toolkit abstracts operations into Docker containers, we need to specify how the Docker containers can access outside data. We do this with a file called `~/.tao_mounts.json` where we specify the mapping from \"source\" directories to \"destination\" directories. See more here: https://docs.nvidia.com/tao/tao-toolkit/text/tao_launcher.html#running-the-launcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "187c244d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "{\r\n",
      "    \"Mounts\": [\r\n",
      "        {\r\n",
      "            \"source\": \"/home/ubuntu/ananth-tao-experiments/project\",\r\n",
      "            \"destination\": \"/workspace/project\"\r\n",
      "        },\r\n",
      "        {\r\n",
      "            \"source\": \"/home/ubuntu/ananth-tao-experiments/project\",\r\n",
      "            \"destination\": \"/root/datasets\"\r\n",
      "        }\r\n",
      "    ],\r\n",
      "    \"DockerOptions\": {\r\n",
      "        \"shm_size\": \"14G\",\r\n",
      "        \"ulimits\": {\r\n",
      "            \"memlock\": -1,\r\n",
      "            \"stack\": 67108864\r\n",
      "        },\r\n",
      "        \"user\": \"1000:1000\"\r\n",
      "    }\r\n",
      "}\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "tao_mounts = \"\"\"\n",
    "{\n",
    "    \"Mounts\": [\n",
    "        {\n",
    "            \"source\": \"/home/ubuntu/ananth-tao-experiments/project\",\n",
    "            \"destination\": \"/workspace/project\"\n",
    "        },\n",
    "        {\n",
    "            \"source\": \"/home/ubuntu/ananth-tao-experiments/project\",\n",
    "            \"destination\": \"/root/datasets\"\n",
    "        }\n",
    "    ],\n",
    "    \"DockerOptions\": {\n",
    "        \"shm_size\": \"14G\",\n",
    "        \"ulimits\": {\n",
    "            \"memlock\": -1,\n",
    "            \"stack\": 67108864\n",
    "        },\n",
    "        \"user\": \"1000:1000\"\n",
    "    }\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# write the tao_mounts config to a JSON file\n",
    "file_object = open('/home/ubuntu/.tao_mounts.json', 'a')\n",
    "file_object.write(tao_mounts)\n",
    "file_object.close()\n",
    "!cat ~/.tao_mounts.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb589b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "Downloading Dataset Version Zip in Aquarium-Combined-3 to coco: 9% [24567808 / 271981707] bytes"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in Aquarium-Combined-3 to coco: 21% [59408384 / 271981707] bytes"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in Aquarium-Combined-3 to coco: 33% [90619904 / 271981707] bytes"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in Aquarium-Combined-3 to coco: 45% [124493824 / 271981707] bytes"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in Aquarium-Combined-3 to coco: 57% [156868608 / 271981707] bytes"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in Aquarium-Combined-3 to coco: 70% [190808064 / 271981707] bytes"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in Aquarium-Combined-3 to coco: 82% [225386496 / 271981707] bytes"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in Aquarium-Combined-3 to coco: 95% [260694016 / 271981707] bytes"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# *Replace this cell with your own snippet from Roboflow Export.*\n",
    "\n",
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"[API_KEY]\")\n",
    "project = rf.workspace(\"brad-dwyer\").project(\"aquarium-combined\")\n",
    "dataset = project.version(3).download(\"coco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75c40f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "coco_config {\r\n",
      "  root_directory_path: \"/workspace/project/Aquarium-Combined-3\"\r\n",
      "  img_dir_names: [\"test\", \"train\", \"valid\"]\r\n",
      "  annotation_files: [\"test/_annotations.coco.json\", \"train/_annotations.coco.json\", \"valid/_annotations.coco.json\"]\r\n",
      "  num_partitions: 3\r\n",
      "  num_shards: [1, 72, 2]\r\n",
      "}\r\n",
      "image_directory_path: \"/workspace/project/Aquarium-Combined-3\"\r\n"
     ]
    }
   ],
   "source": [
    "# Specify RF COCO dataset details\n",
    "\n",
    "dataset_config = \"\"\"\n",
    "coco_config {\n",
    "  root_directory_path: \"/workspace/project/Aquarium-Combined-3\"\n",
    "  img_dir_names: [\"test\", \"train\", \"valid\"]\n",
    "  annotation_files: [\"test/_annotations.coco.json\", \"train/_annotations.coco.json\", \"valid/_annotations.coco.json\"]\n",
    "  num_partitions: 3\n",
    "  num_shards: [1, 72, 2]\n",
    "}\n",
    "image_directory_path: \"/workspace/project/Aquarium-Combined-3\"\n",
    "\"\"\"\n",
    "\n",
    "# write the dataset config to a JSON file\n",
    "file_object = open('project/coco_config.json', 'a')\n",
    "file_object.write(dataset_config)\n",
    "file_object.close()\n",
    "!cat ./project/coco_config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87583200",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.6) or chardet (4.0.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "2022-08-11 02:13:43,498 [INFO] root: Registry: ['nvcr.io']\n",
      "2022-08-11 02:13:43,559 [INFO] tlt.components.instance_handler.local_instance: Running command in container: nvcr.io/nvidia/tao/tao-toolkit-tf:v3.22.05-tf1.15.5-py3\n",
      "Matplotlib created a temporary config/cache directory at /tmp/matplotlib-lvdrarun because the default path (/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n",
      "Using TensorFlow backend.\n",
      "WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.\n",
      "/usr/local/lib/python3.6/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.5) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n",
      "Using TensorFlow backend.\n",
      "2022-08-11 02:13:52,539 [INFO] iva.detectnet_v2.dataio.build_converter: Instantiating a coco converter\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.17s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "2022-08-11 02:13:52,735 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 0, shard 0\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/dataio/coco_converter_lib.py:136: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
      "\n",
      "2022-08-11 02:13:52,735 [WARNING] tensorflow: From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/dataio/coco_converter_lib.py:136: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
      "\n",
      "2022-08-11 02:13:52,755 [INFO] iva.detectnet_v2.dataio.dataset_converter_lib: \n",
      "Wrote the following numbers of objects:\n",
      "b'stingray': 15\n",
      "b'fish': 249\n",
      "b'starfish': 11\n",
      "b'jellyfish': 154\n",
      "b'penguin': 82\n",
      "b'shark': 36\n",
      "b'puffin': 35\n",
      "\n",
      "2022-08-11 02:13:52,756 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 0\n",
      "2022-08-11 02:13:52,774 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 1\n",
      "2022-08-11 02:13:52,791 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 2\n",
      "2022-08-11 02:13:52,810 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 3\n",
      "2022-08-11 02:13:52,829 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 4\n",
      "2022-08-11 02:13:52,847 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 5\n",
      "2022-08-11 02:13:52,865 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 6\n",
      "2022-08-11 02:13:52,883 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 7\n",
      "2022-08-11 02:13:52,900 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 8\n",
      "2022-08-11 02:13:52,919 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 9\n",
      "2022-08-11 02:13:52,937 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 10\n",
      "2022-08-11 02:13:52,956 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 11\n",
      "2022-08-11 02:13:52,973 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 12\n",
      "2022-08-11 02:13:52,992 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 13\n",
      "2022-08-11 02:13:53,010 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 14\n",
      "2022-08-11 02:13:53,027 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 15\n",
      "2022-08-11 02:13:53,046 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 16\n",
      "2022-08-11 02:13:53,064 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 17\n",
      "2022-08-11 02:13:53,082 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 18\n",
      "2022-08-11 02:13:53,099 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 19\n",
      "2022-08-11 02:13:53,117 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 20\n",
      "2022-08-11 02:13:53,134 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 21\n",
      "2022-08-11 02:13:53,152 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 22\n",
      "2022-08-11 02:13:53,171 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 23\n",
      "2022-08-11 02:13:53,188 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 24\n",
      "2022-08-11 02:13:53,206 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 25\n",
      "2022-08-11 02:13:53,225 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 26\n",
      "2022-08-11 02:13:53,242 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 27\n",
      "2022-08-11 02:13:53,260 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 28\n",
      "2022-08-11 02:13:53,278 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 29\n",
      "2022-08-11 02:13:53,296 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 30\n",
      "2022-08-11 02:13:53,314 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 31\n",
      "2022-08-11 02:13:53,333 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 32\n",
      "2022-08-11 02:13:53,351 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 33\n",
      "2022-08-11 02:13:53,369 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 34\n",
      "2022-08-11 02:13:53,388 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 35\n",
      "2022-08-11 02:13:53,406 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 36\n",
      "2022-08-11 02:13:53,424 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 37\n",
      "2022-08-11 02:13:53,443 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 38\n",
      "2022-08-11 02:13:53,460 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 39\n",
      "2022-08-11 02:13:53,479 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 40\n",
      "2022-08-11 02:13:53,497 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 41\n",
      "2022-08-11 02:13:53,515 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 42\n",
      "2022-08-11 02:13:53,534 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 43\n",
      "2022-08-11 02:13:53,552 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 44\n",
      "2022-08-11 02:13:53,571 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 45\n",
      "2022-08-11 02:13:53,588 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 46\n",
      "2022-08-11 02:13:53,606 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 47\n",
      "2022-08-11 02:13:53,624 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 48\n",
      "2022-08-11 02:13:53,642 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 49\n",
      "2022-08-11 02:13:53,660 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 50\n",
      "2022-08-11 02:13:53,678 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 51\n",
      "2022-08-11 02:13:53,696 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 52\n",
      "2022-08-11 02:13:53,714 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 53\n",
      "2022-08-11 02:13:53,732 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 54\n",
      "2022-08-11 02:13:53,750 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 55\n",
      "2022-08-11 02:13:53,770 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 56\n",
      "2022-08-11 02:13:53,789 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 57\n",
      "2022-08-11 02:13:53,807 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 58\n",
      "2022-08-11 02:13:53,827 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 59\n",
      "2022-08-11 02:13:53,846 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 60\n",
      "2022-08-11 02:13:53,864 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 61\n",
      "2022-08-11 02:13:53,883 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 62\n",
      "2022-08-11 02:13:53,901 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 63\n",
      "2022-08-11 02:13:53,920 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 64\n",
      "2022-08-11 02:13:53,938 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 65\n",
      "2022-08-11 02:13:53,956 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 66\n",
      "2022-08-11 02:13:53,974 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 67\n",
      "2022-08-11 02:13:53,992 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 68\n",
      "2022-08-11 02:13:54,011 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 69\n",
      "2022-08-11 02:13:54,029 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 70\n",
      "2022-08-11 02:13:54,047 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 1, shard 71\n",
      "2022-08-11 02:13:54,070 [INFO] iva.detectnet_v2.dataio.dataset_converter_lib: \n",
      "Wrote the following numbers of objects:\n",
      "b'shark': 2584\n",
      "b'fish': 19213\n",
      "b'stingray': 1349\n",
      "b'puffin': 1740\n",
      "b'starfish': 779\n",
      "b'jellyfish': 3742\n",
      "b'penguin': 3220\n",
      "\n",
      "2022-08-11 02:13:54,070 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 2, shard 0\n",
      "2022-08-11 02:13:54,089 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Writing partition 2, shard 1\n",
      "2022-08-11 02:13:54,107 [INFO] iva.detectnet_v2.dataio.dataset_converter_lib: \n",
      "Wrote the following numbers of objects:\n",
      "b'starfish': 27\n",
      "b'stingray': 33\n",
      "b'penguin': 104\n",
      "b'puffin': 74\n",
      "b'shark': 57\n",
      "b'fish': 459\n",
      "b'jellyfish': 155\n",
      "\n",
      "2022-08-11 02:13:54,107 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Cumulative object statistics\n",
      "2022-08-11 02:13:54,107 [INFO] iva.detectnet_v2.dataio.dataset_converter_lib: \n",
      "Wrote the following numbers of objects:\n",
      "b'stingray': 1397\n",
      "b'fish': 19921\n",
      "b'starfish': 817\n",
      "b'jellyfish': 4051\n",
      "b'penguin': 3406\n",
      "b'shark': 2677\n",
      "b'puffin': 1849\n",
      "\n",
      "2022-08-11 02:13:54,107 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Class map. \n",
      "Label in GT: Label in tfrecords file \n",
      "stingray: stingray\n",
      "fish: fish\n",
      "starfish: starfish\n",
      "jellyfish: jellyfish\n",
      "penguin: penguin\n",
      "shark: shark\n",
      "puffin: puffin\n",
      "For the dataset_config in the experiment_spec, please use labels in the tfrecords file, while writing the classmap.\n",
      "\n",
      "2022-08-11 02:13:54,107 [INFO] iva.detectnet_v2.dataio.coco_converter_lib: Tfrecords generation complete.\n",
      "2022-08-11 02:13:54,108 [INFO] iva.detectnet_v2.dataio.dataset_converter_lib: Writing the log_warning.json\n",
      "2022-08-11 02:13:54,108 [INFO] iva.detectnet_v2.dataio.dataset_converter_lib: There were errors in the labels. Details are logged at /workspace/project/output.tfrecords_waring.json\n",
      "2022-08-11 02:13:55,230 [INFO] tlt.components.docker_handler.docker_handler: Stopping container.\n"
     ]
    }
   ],
   "source": [
    "# Convert to TFRecords. We'll move the TFRecord files to a custom directory\n",
    "# in the next cell.\n",
    "\n",
    "!/home/ubuntu/.local/bin/tao yolo_v4 dataset_convert -d \"/workspace/project/coco_config.json\" --gpu_index 0 -o \"/workspace/project/output.tfrecords\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f56bb490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘project/output_tfrecords’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir project/output_tfrecords\n",
    "!mv project/output.tfrecords* project/output_tfrecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e8e8f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-08-11 03:00:48--  https://api.ngc.nvidia.com/v2/models/nvidia/tao/pretrained_object_detection/versions/resnet18/files/resnet_18.hdf5\n",
      "Resolving api.ngc.nvidia.com (api.ngc.nvidia.com)... 44.239.119.156, 54.213.148.69\n",
      "Connecting to api.ngc.nvidia.com (api.ngc.nvidia.com)|44.239.119.156|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 \n",
      "Location: https://prod-model-registry-ngc-bucket.s3.us-west-2.amazonaws.com/org/nvidia/team/tao/models/pretrained_object_detection/versions/resnet18/files/resnet_18.hdf5?response-content-disposition=attachment%3B%20filename%3D%22resnet_18.hdf5%22&response-content-type=application%2Foctet-stream&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEOv%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLXdlc3QtMiJIMEYCIQCNMBBmXVzvQnoFv%2F9ZFq3xG9vkR5OmUv3kYlt%2BKnyQVQIhAKd12ahSN22qzvh4vrBGwG022f5v7YETkdjFrkSb2%2BPRKtIECFQQBBoMNzg5MzYzMTM1MDI3IgzAtGKO0hyuEcw5BMAqrwSsMpNWDvxUWOy4zhf%2B4iNg7nxn%2Bwgt%2B6rTbTdtBqA15%2BKP8aII56u8bZXQ8zySyW8R9qUQQphq7JekBb8FygBy1dj3DxYhuz%2FN7eo%2BjcWNmbgjHqamOWqosy55ftj8l%2Fnqbdu%2BDOJvZcVYkUYppjIwvzmgVlbmTbkdJHzOM0frDZcVZ95bNAFehvDPZMvdf3y2Fdg4GqdROK9LZQs7ISr1XfWhwLBFGNVXFeKFDkCzuUh3KDXMZikJE7P7RIMqA5k6PrgJiYII18c%2B5FG4tbboDnalrluQj7ZA3bnl5PB8IGVNIIyCPLq0ENxsKohPA3NV39P0S40zsD7kQtijgZjUa6th4qJVnufEvrY%2BT0%2F2HZ%2BODeg2hHjDwTZcbiiSEhy5gy82%2By1W6pv30Kfegn3B8lQCfAjT34Mb2Mf1l2vJZZ9%2BwId3MbteI%2FVl54j15D%2BXfi3%2BT8heYw%2BF4%2FqlQUcLbOSmpu0dFRz17nKVEpkIw85j9G%2BYMHmlcXrG5IT%2B8Uiwai%2F2zNpHuVlQ3rfSaIKSkude8DxEXNcdM3s0UjMr5nezaCRsrgVdngjilxVCIb342p6QrwlHZq01oRvwNfWT6ZgD5JozWzLGSHgWlVLTgEJir0Lmtct%2FtHwA%2F3Gj0yKF%2FC1ImCHa3KhIxmi8Fix9HFu0bjHqWvpsnyGwdMdnJhgCl1pcYBAho3A1hHTP1JOGAxao%2B47tsIZpw%2BlaoL7xWgXHktlH1vksfBwRuYdVMPPg0ZcGOqgB5ws7na%2BY8uGr6f0uowP5rfWK8jUsCdPOcj26zAABd8N2pg2EwgTq87eDdkIGfAUAvcT3IcTcYQ9DGynLY6xdmxbBJmt%2FafogMI5%2BZ4s3sNELzcc8TOGnbcCUlM8t0R5oG%2BOgZRUD8aw6OxUaWWHW7cQqoy1jrySIoxy0Gx6N44q1x2U4U0Grd9VO9j5rCUKUrscjsK%2F0fbJMrzn3TCrEZPbF7Wj41RV9&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20220811T030049Z&X-Amz-SignedHeaders=host&X-Amz-Expires=3599&X-Amz-Credential=ASIA3PSNVSIZZMMHA6XI%2F20220811%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Signature=79ba1f8887c86c1498c85d299cb38bc4a467b4c821edb504cc3378112c024dd3 [following]\n",
      "--2022-08-11 03:00:49--  https://prod-model-registry-ngc-bucket.s3.us-west-2.amazonaws.com/org/nvidia/team/tao/models/pretrained_object_detection/versions/resnet18/files/resnet_18.hdf5?response-content-disposition=attachment%3B%20filename%3D%22resnet_18.hdf5%22&response-content-type=application%2Foctet-stream&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEOv%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLXdlc3QtMiJIMEYCIQCNMBBmXVzvQnoFv%2F9ZFq3xG9vkR5OmUv3kYlt%2BKnyQVQIhAKd12ahSN22qzvh4vrBGwG022f5v7YETkdjFrkSb2%2BPRKtIECFQQBBoMNzg5MzYzMTM1MDI3IgzAtGKO0hyuEcw5BMAqrwSsMpNWDvxUWOy4zhf%2B4iNg7nxn%2Bwgt%2B6rTbTdtBqA15%2BKP8aII56u8bZXQ8zySyW8R9qUQQphq7JekBb8FygBy1dj3DxYhuz%2FN7eo%2BjcWNmbgjHqamOWqosy55ftj8l%2Fnqbdu%2BDOJvZcVYkUYppjIwvzmgVlbmTbkdJHzOM0frDZcVZ95bNAFehvDPZMvdf3y2Fdg4GqdROK9LZQs7ISr1XfWhwLBFGNVXFeKFDkCzuUh3KDXMZikJE7P7RIMqA5k6PrgJiYII18c%2B5FG4tbboDnalrluQj7ZA3bnl5PB8IGVNIIyCPLq0ENxsKohPA3NV39P0S40zsD7kQtijgZjUa6th4qJVnufEvrY%2BT0%2F2HZ%2BODeg2hHjDwTZcbiiSEhy5gy82%2By1W6pv30Kfegn3B8lQCfAjT34Mb2Mf1l2vJZZ9%2BwId3MbteI%2FVl54j15D%2BXfi3%2BT8heYw%2BF4%2FqlQUcLbOSmpu0dFRz17nKVEpkIw85j9G%2BYMHmlcXrG5IT%2B8Uiwai%2F2zNpHuVlQ3rfSaIKSkude8DxEXNcdM3s0UjMr5nezaCRsrgVdngjilxVCIb342p6QrwlHZq01oRvwNfWT6ZgD5JozWzLGSHgWlVLTgEJir0Lmtct%2FtHwA%2F3Gj0yKF%2FC1ImCHa3KhIxmi8Fix9HFu0bjHqWvpsnyGwdMdnJhgCl1pcYBAho3A1hHTP1JOGAxao%2B47tsIZpw%2BlaoL7xWgXHktlH1vksfBwRuYdVMPPg0ZcGOqgB5ws7na%2BY8uGr6f0uowP5rfWK8jUsCdPOcj26zAABd8N2pg2EwgTq87eDdkIGfAUAvcT3IcTcYQ9DGynLY6xdmxbBJmt%2FafogMI5%2BZ4s3sNELzcc8TOGnbcCUlM8t0R5oG%2BOgZRUD8aw6OxUaWWHW7cQqoy1jrySIoxy0Gx6N44q1x2U4U0Grd9VO9j5rCUKUrscjsK%2F0fbJMrzn3TCrEZPbF7Wj41RV9&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20220811T030049Z&X-Amz-SignedHeaders=host&X-Amz-Expires=3599&X-Amz-Credential=ASIA3PSNVSIZZMMHA6XI%2F20220811%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Signature=79ba1f8887c86c1498c85d299cb38bc4a467b4c821edb504cc3378112c024dd3\n",
      "Resolving prod-model-registry-ngc-bucket.s3.us-west-2.amazonaws.com (prod-model-registry-ngc-bucket.s3.us-west-2.amazonaws.com)... 52.218.225.73\n",
      "Connecting to prod-model-registry-ngc-bucket.s3.us-west-2.amazonaws.com (prod-model-registry-ngc-bucket.s3.us-west-2.amazonaws.com)|52.218.225.73|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 93278448 (89M) [application/octet-stream]\n",
      "Saving to: ‘resnet_18.hdf5’\n",
      "\n",
      "resnet_18.hdf5      100%[===================>]  88.96M  29.7MB/s    in 3.0s    \n",
      "\n",
      "2022-08-11 03:00:53 (29.7 MB/s) - ‘resnet_18.hdf5’ saved [93278448/93278448]\n",
      "\n",
      "--2022-08-11 03:00:53--  http://./project/resnet_18.hdf5\n",
      "Resolving . (.)... failed: Temporary failure in name resolution.\n",
      "wget: unable to resolve host address ‘.’\n",
      "FINISHED --2022-08-11 03:00:53--\n",
      "Total wall clock time: 4.8s\n",
      "Downloaded: 1 files, 89M in 3.0s (29.7 MB/s)\n"
     ]
    }
   ],
   "source": [
    "# Download NVIDIA pretrained Resnet18 model.\n",
    "\n",
    "!wget https://api.ngc.nvidia.com/v2/models/nvidia/tao/pretrained_object_detection/versions/resnet18/files/resnet_18.hdf5 ./project/resnet_18.hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8868a2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "random_seed: 42\r\n",
      "yolov4_config {\r\n",
      "  big_anchor_shape: \"[(114.94, 60.67), (159.06, 114.59), (297.59, 176.38)]\"\r\n",
      "  mid_anchor_shape: \"[(42.99, 31.91), (79.57, 31.75), (56.80, 56.93)]\"\r\n",
      "  small_anchor_shape: \"[(15.60, 13.88), (30.25, 20.25), (20.67, 49.63)]\"\r\n",
      "  box_matching_iou: 0.25\r\n",
      "  matching_neutral_box_iou: 0.5\r\n",
      "  arch: \"resnet\"\r\n",
      "  nlayers: 18\r\n",
      "  loss_loc_weight: 1.0\r\n",
      "  loss_neg_obj_weights: 1.0\r\n",
      "  loss_class_weights: 1.0\r\n",
      "  label_smoothing: 0.0\r\n",
      "  big_grid_xy_extend: 0.05\r\n",
      "  mid_grid_xy_extend: 0.1\r\n",
      "  small_grid_xy_extend: 0.2\r\n",
      "  freeze_bn: false\r\n",
      "  freeze_blocks: 0\r\n",
      "  force_relu: false\r\n",
      "}\r\n",
      "training_config {\r\n",
      "  batch_size_per_gpu: 8\r\n",
      "  num_epochs: 80\r\n",
      "  enable_qat: false\r\n",
      "  checkpoint_interval: 10\r\n",
      "  learning_rate {\r\n",
      "    soft_start_cosine_annealing_schedule {\r\n",
      "      min_learning_rate: 1e-7\r\n",
      "      max_learning_rate: 1e-4\r\n",
      "      soft_start: 0.3\r\n",
      "    }\r\n",
      "  }\r\n",
      "  regularizer {\r\n",
      "    type: L1\r\n",
      "    weight: 3e-5\r\n",
      "  }\r\n",
      "  optimizer {\r\n",
      "    adam {\r\n",
      "      epsilon: 1e-7\r\n",
      "      beta1: 0.9\r\n",
      "      beta2: 0.999\r\n",
      "      amsgrad: false\r\n",
      "    }\r\n",
      "  }\r\n",
      "  pretrain_model_path: \"/workspace/project/resnet_18.hdf5\"\r\n",
      "}\r\n",
      "eval_config {\r\n",
      "  average_precision_mode: SAMPLE\r\n",
      "  batch_size: 8\r\n",
      "  matching_iou_threshold: 0.5\r\n",
      "}\r\n",
      "nms_config {\r\n",
      "  confidence_threshold: 0.001\r\n",
      "  clustering_iou_threshold: 0.5\r\n",
      "  top_k: 200\r\n",
      "}\r\n",
      "augmentation_config {\r\n",
      "  hue: 0\r\n",
      "  saturation: 1\r\n",
      "  exposure: 1\r\n",
      "  vertical_flip: 0\r\n",
      "  horizontal_flip: 0\r\n",
      "  jitter: 0\r\n",
      "  output_width: 640\r\n",
      "  output_height: 640\r\n",
      "  output_channel: 3\r\n",
      "  randomize_input_shape_period: 0\r\n",
      "  mosaic_prob: 0\r\n",
      "  mosaic_min_ratio: 0.2\r\n",
      "}\r\n",
      "dataset_config {\r\n",
      "  data_sources: {\r\n",
      "      tfrecords_path: \"/workspace/project/output_tfrecords/*\"\r\n",
      "      image_directory_path: \"/workspace/project/Aquarium-Combined-3\"\r\n",
      "  }\r\n",
      "  include_difficult_in_training: true\r\n",
      "  image_extension: \"jpg\"\r\n",
      "  target_class_mapping {\r\n",
      "      key: \"fish\"\r\n",
      "      value: \"fish\"\r\n",
      "  }\r\n",
      "  target_class_mapping {\r\n",
      "      key: \"jellyfish\"\r\n",
      "      value: \"jellyfish\"\r\n",
      "  }\r\n",
      "  target_class_mapping {\r\n",
      "      key: \"penguins\"\r\n",
      "      value: \"penguins\"\r\n",
      "  }\r\n",
      "  target_class_mapping {\r\n",
      "      key: \"sharks\"\r\n",
      "      value: \"sharks\"\r\n",
      "  }\r\n",
      "  target_class_mapping {\r\n",
      "      key: \"puffins\"\r\n",
      "      value: \"puffins\"\r\n",
      "  }\r\n",
      "  target_class_mapping {\r\n",
      "      key: \"stingrays\"\r\n",
      "      value: \"stingrays\"\r\n",
      "  }\r\n",
      "\r\n",
      "  target_class_mapping {\r\n",
      "      key: \"starfish\"\r\n",
      "      value: \"starfish\"\r\n",
      "  }\r\n",
      "  validation_fold: 0\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "experiment_spec = \"\"\"\n",
    "random_seed: 42\n",
    "yolov4_config {\n",
    "  big_anchor_shape: \"[(114.94, 60.67), (159.06, 114.59), (297.59, 176.38)]\"\n",
    "  mid_anchor_shape: \"[(42.99, 31.91), (79.57, 31.75), (56.80, 56.93)]\"\n",
    "  small_anchor_shape: \"[(15.60, 13.88), (30.25, 20.25), (20.67, 49.63)]\"\n",
    "  box_matching_iou: 0.25\n",
    "  matching_neutral_box_iou: 0.5\n",
    "  arch: \"resnet\"\n",
    "  nlayers: 18\n",
    "  loss_loc_weight: 1.0\n",
    "  loss_neg_obj_weights: 1.0\n",
    "  loss_class_weights: 1.0\n",
    "  label_smoothing: 0.0\n",
    "  big_grid_xy_extend: 0.05\n",
    "  mid_grid_xy_extend: 0.1\n",
    "  small_grid_xy_extend: 0.2\n",
    "  freeze_bn: false\n",
    "  freeze_blocks: 0\n",
    "  force_relu: false\n",
    "}\n",
    "training_config {\n",
    "  batch_size_per_gpu: 8\n",
    "  num_epochs: 80\n",
    "  enable_qat: false\n",
    "  checkpoint_interval: 10\n",
    "  learning_rate {\n",
    "    soft_start_cosine_annealing_schedule {\n",
    "      min_learning_rate: 1e-7\n",
    "      max_learning_rate: 1e-4\n",
    "      soft_start: 0.3\n",
    "    }\n",
    "  }\n",
    "  regularizer {\n",
    "    type: L1\n",
    "    weight: 3e-5\n",
    "  }\n",
    "  optimizer {\n",
    "    adam {\n",
    "      epsilon: 1e-7\n",
    "      beta1: 0.9\n",
    "      beta2: 0.999\n",
    "      amsgrad: false\n",
    "    }\n",
    "  }\n",
    "  pretrain_model_path: \"/workspace/project/resnet_18.hdf5\"\n",
    "}\n",
    "eval_config {\n",
    "  average_precision_mode: SAMPLE\n",
    "  batch_size: 8\n",
    "  matching_iou_threshold: 0.5\n",
    "}\n",
    "nms_config {\n",
    "  confidence_threshold: 0.001\n",
    "  clustering_iou_threshold: 0.5\n",
    "  top_k: 200\n",
    "}\n",
    "augmentation_config {\n",
    "  hue: 0\n",
    "  saturation: 1\n",
    "  exposure: 1\n",
    "  vertical_flip: 0\n",
    "  horizontal_flip: 0\n",
    "  jitter: 0\n",
    "  output_width: 640\n",
    "  output_height: 640\n",
    "  output_channel: 3\n",
    "  randomize_input_shape_period: 0\n",
    "  mosaic_prob: 0\n",
    "  mosaic_min_ratio: 0.2\n",
    "}\n",
    "dataset_config {\n",
    "  data_sources: {\n",
    "      tfrecords_path: \"/workspace/project/output_tfrecords/*\"\n",
    "      image_directory_path: \"/workspace/project/Aquarium-Combined-3\"\n",
    "  }\n",
    "  include_difficult_in_training: true\n",
    "  image_extension: \"jpg\"\n",
    "  target_class_mapping {\n",
    "      key: \"fish\"\n",
    "      value: \"fish\"\n",
    "  }\n",
    "  target_class_mapping {\n",
    "      key: \"jellyfish\"\n",
    "      value: \"jellyfish\"\n",
    "  }\n",
    "  target_class_mapping {\n",
    "      key: \"penguins\"\n",
    "      value: \"penguins\"\n",
    "  }\n",
    "  target_class_mapping {\n",
    "      key: \"sharks\"\n",
    "      value: \"sharks\"\n",
    "  }\n",
    "  target_class_mapping {\n",
    "      key: \"puffins\"\n",
    "      value: \"puffins\"\n",
    "  }\n",
    "  target_class_mapping {\n",
    "      key: \"stingrays\"\n",
    "      value: \"stingrays\"\n",
    "  }\n",
    "\n",
    "  target_class_mapping {\n",
    "      key: \"starfish\"\n",
    "      value: \"starfish\"\n",
    "  }\n",
    "  validation_fold: 0\n",
    "}\n",
    "\"\"\"\n",
    "# write the configs to a JSON file\n",
    "file_object = open('project/experiment_spec.json', 'a')\n",
    "file_object.write(experiment_spec)\n",
    "file_object.close()\n",
    "!cat ./project/experiment_spec.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd784cee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.6) or chardet (4.0.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "2022-08-11 04:24:21,386 [INFO] root: Registry: ['nvcr.io']\n",
      "2022-08-11 04:24:21,483 [INFO] tlt.components.instance_handler.local_instance: Running command in container: nvcr.io/nvidia/tao/tao-toolkit-tf:v3.22.05-tf1.15.5-py3\n",
      "Matplotlib created a temporary config/cache directory at /tmp/matplotlib-pwyubmn_ because the default path (/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n",
      "Using TensorFlow backend.\n",
      "Using TensorFlow backend.\n",
      "WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.\n",
      "/usr/local/lib/python3.6/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.5) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/yolo_v4/scripts/train.py:42: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING: From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/yolo_v4/scripts/train.py:42: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/yolo_v4/scripts/train.py:45: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING: From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/yolo_v4/scripts/train.py:45: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:153: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:153: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/nvidia/third_party/keras/tensorflow_backend.py:183: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING: From /opt/nvidia/third_party/keras/tensorflow_backend.py:183: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2018: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
      "\n",
      "WARNING: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2018: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
      "\n",
      "INFO: Serial augmentation enabled = False\n",
      "INFO: Pseudo sharding enabled = False\n",
      "INFO: Max Image Dimensions (all sources): (0, 0)\n",
      "INFO: number of cpus: 4, io threads: 8, compute threads: 4, buffered batches: -1\n",
      "INFO: total dataset size 4607, number of sources: 1, batch size per gpu: 8, steps: 576\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n",
      "WARNING: From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n",
      "WARNING:tensorflow:Entity <bound method YOLOv3TFRecordsParser.__call__ of <iva.yolo_v3.data_loader.yolo_v3_data_loader.YOLOv3TFRecordsParser object at 0x7f5830c63748>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method YOLOv3TFRecordsParser.__call__ of <iva.yolo_v3.data_loader.yolo_v3_data_loader.YOLOv3TFRecordsParser object at 0x7f5830c63748>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "WARNING: Entity <bound method YOLOv3TFRecordsParser.__call__ of <iva.yolo_v3.data_loader.yolo_v3_data_loader.YOLOv3TFRecordsParser object at 0x7f5830c63748>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method YOLOv3TFRecordsParser.__call__ of <iva.yolo_v3.data_loader.yolo_v3_data_loader.YOLOv3TFRecordsParser object at 0x7f5830c63748>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "INFO: Bounding box coordinates were detected in the input specification! Bboxes will be automatically converted to polygon coordinates.\n",
      "INFO: shuffle: True - shard 0 of 1\n",
      "INFO: sampling 1 datasets with weights:\n",
      "INFO: source: 0 weight: 1.000000\n",
      "WARNING:tensorflow:Entity <bound method Processor.__call__ of <modulus.blocks.data_loaders.multi_source_loader.processors.asset_loader.AssetLoader object at 0x7f5830a1da58>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method Processor.__call__ of <modulus.blocks.data_loaders.multi_source_loader.processors.asset_loader.AssetLoader object at 0x7f5830a1da58>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "WARNING: Entity <bound method Processor.__call__ of <modulus.blocks.data_loaders.multi_source_loader.processors.asset_loader.AssetLoader object at 0x7f5830a1da58>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method Processor.__call__ of <modulus.blocks.data_loaders.multi_source_loader.processors.asset_loader.AssetLoader object at 0x7f5830a1da58>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "/opt/nvidia/third_party/keras/tensorflow_backend.py:356: UserWarning: Seed 42 from outer graph might be getting used by function Dataset_map__map_func_set_random_wrapper, if the random op has not been provided any seed. Explicitly set the seed in the function if this is not the intended behavior.\n",
      "  self, _map_func_set_random_wrapper, num_parallel_calls=num_parallel_calls\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/yolo_v4/dataio/tf_data_pipe.py:43: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\n",
      "\n",
      "WARNING: From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/yolo_v4/dataio/tf_data_pipe.py:43: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/nvidia/third_party/keras/tensorflow_backend.py:187: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n",
      "WARNING: From /opt/nvidia/third_party/keras/tensorflow_backend.py:187: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING: From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "INFO: Serial augmentation enabled = False\n",
      "INFO: Pseudo sharding enabled = False\n",
      "INFO: Max Image Dimensions (all sources): (0, 0)\n",
      "INFO: number of cpus: 4, io threads: 8, compute threads: 4, buffered batches: -1\n",
      "INFO: total dataset size 63, number of sources: 1, batch size per gpu: 8, steps: 8\n",
      "WARNING:tensorflow:Entity <bound method YOLOv3TFRecordsParser.__call__ of <iva.yolo_v3.data_loader.yolo_v3_data_loader.YOLOv3TFRecordsParser object at 0x7f580226a860>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method YOLOv3TFRecordsParser.__call__ of <iva.yolo_v3.data_loader.yolo_v3_data_loader.YOLOv3TFRecordsParser object at 0x7f580226a860>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "WARNING: Entity <bound method YOLOv3TFRecordsParser.__call__ of <iva.yolo_v3.data_loader.yolo_v3_data_loader.YOLOv3TFRecordsParser object at 0x7f580226a860>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method YOLOv3TFRecordsParser.__call__ of <iva.yolo_v3.data_loader.yolo_v3_data_loader.YOLOv3TFRecordsParser object at 0x7f580226a860>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "INFO: Bounding box coordinates were detected in the input specification! Bboxes will be automatically converted to polygon coordinates.\n",
      "INFO: shuffle: False - shard 0 of 1\n",
      "INFO: sampling 1 datasets with weights:\n",
      "INFO: source: 0 weight: 1.000000\n",
      "WARNING:tensorflow:Entity <bound method Processor.__call__ of <modulus.blocks.data_loaders.multi_source_loader.processors.asset_loader.AssetLoader object at 0x7f580208db00>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method Processor.__call__ of <modulus.blocks.data_loaders.multi_source_loader.processors.asset_loader.AssetLoader object at 0x7f580208db00>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "WARNING: Entity <bound method Processor.__call__ of <modulus.blocks.data_loaders.multi_source_loader.processors.asset_loader.AssetLoader object at 0x7f580208db00>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method Processor.__call__ of <modulus.blocks.data_loaders.multi_source_loader.processors.asset_loader.AssetLoader object at 0x7f580208db00>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n"
     ]
    }
   ],
   "source": [
    "# Train our model\n",
    "\n",
    "!tao yolo_v4 train -e \"/workspace/project/experiment_spec.json\" -r \"/workspace/project/run2\" -k \"roboflow\" --use_amp\n",
    "\n",
    "# Logs will be saved to the \"run1\" directory, including a CSV of \n",
    "# training accuracy over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bab8fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.6) or chardet (4.0.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "2022-08-11 06:53:15,097 [INFO] root: Registry: ['nvcr.io']\n",
      "2022-08-11 06:53:15,178 [INFO] tlt.components.instance_handler.local_instance: Running command in container: nvcr.io/nvidia/tao/tao-toolkit-tf:v3.22.05-tf1.15.5-py3\n",
      "Matplotlib created a temporary config/cache directory at /tmp/matplotlib-2b6eji1g because the default path (/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n",
      "Using TensorFlow backend.\n",
      "Using TensorFlow backend.\n",
      "WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.\n",
      "/usr/local/lib/python3.6/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.5) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:95: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "2022-08-11 06:53:25,495 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:95: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:98: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "2022-08-11 06:53:25,496 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:98: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:102: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "2022-08-11 06:53:25,500 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:102: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "2022-08-11 06:53:29,399 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "2022-08-11 06:53:29,440 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "2022-08-11 06:53:29,469 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/nvidia/third_party/keras/tensorflow_backend.py:183: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "2022-08-11 06:53:30,227 [WARNING] tensorflow: From /opt/nvidia/third_party/keras/tensorflow_backend.py:183: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2018: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
      "\n",
      "2022-08-11 06:53:30,402 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2018: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "2022-08-11 06:53:31,831 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "2022-08-11 06:53:31,831 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "2022-08-11 06:53:31,832 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "2022-08-11 06:53:32,698 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "2022-08-11 06:53:32,699 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "2022-08-11 06:53:33,527 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "2022-08-11 06:53:35,072 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/common/losses/base_loss.py:40: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "2022-08-11 06:53:35,133 [WARNING] tensorflow: From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/common/losses/base_loss.py:40: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "2022-08-11 06:53:38,119 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "2022-08-11 06:53:39,531 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Using TLT model for inference, setting batch size to the one in eval_config: 8\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:15<00:00,  1.96s/it]\n",
      "2022-08-11 06:54:07,661 [INFO] tlt.components.docker_handler.docker_handler: Stopping container.\n"
     ]
    }
   ],
   "source": [
    "# Run inference with our trained model.\n",
    "\n",
    "!tao yolo_v4 inference -i \"/workspace/project/Aquarium-Combined-3/test\" -o \"/workspace/project/Aquarium-Combined-3/test_output\" -e \"/workspace/project/experiment_spec.json\" -m \"/workspace/project/run1/weights/yolov4_resnet18_epoch_040.tlt\" -k \"roboflow\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
